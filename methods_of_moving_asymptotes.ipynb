{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nlopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 4th order polynomial function\n",
    "def polynomial(x):\n",
    "    return x**4 - 5*x**2 + x + 3\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(-3, 3, 500)\n",
    "\n",
    "# Plot the polynomial\n",
    "fig, ax = plt.subplots(1,1,figsize=(5, 4))\n",
    "ax.plot(x, polynomial(x), label='4th Order Polynomial')\n",
    "ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "ax.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "ax.set_title(\"4th Order Polynomial with 2 Minima\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(x, grad):\n",
    "    if grad.size > 0:\n",
    "        grad[0] = 4 * x[0]**3 - 10 * x[0] + 1  # Gradient of the polynomial\n",
    "    return x[0]**4 - 5 * x[0]**2 + x[0] + 3  # Polynomial function\n",
    "\n",
    "# Create an optimizer object\n",
    "opt = nlopt.opt(nlopt.LD_MMA, 1)  # LD_MMA for Method of Moving Asymptotes\n",
    "\n",
    "# Set the objective function\n",
    "opt.set_min_objective(objective)\n",
    "# Set relative tolerance\n",
    "opt.set_xtol_rel(1e-6)\n",
    "\n",
    "# Set bounds\n",
    "x_min = [-2.0]\n",
    "x_max = [2.0]\n",
    "opt.set_lower_bounds(x_min)\n",
    "opt.set_upper_bounds(x_max)\n",
    "\n",
    "# Set initial guess\n",
    "x0 = [0.0]\n",
    "\n",
    "# Optimize\n",
    "x_opt = opt.optimize(x0)\n",
    "min_f = opt.last_optimum_value()\n",
    "\n",
    "print(f\"Optimal solution: x = {x_opt[0]}\")\n",
    "print(f\"Minimum value of the polynomial: f(x) = {min_f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the progress of x during optimization\n",
    "x_progress = []\n",
    "\n",
    "# Define a callback function to record x values\n",
    "def progress_callback(x, grad):\n",
    "    x_progress.append(x[0])\n",
    "    return objective(x, grad)\n",
    "\n",
    "# Set the callback function in the optimizer\n",
    "opt.set_min_objective(progress_callback)\n",
    "\n",
    "# Re-run the optimization\n",
    "x_progress.clear()  # Clear previous progress\n",
    "x_opt = opt.optimize(x0)\n",
    "min_f = opt.last_optimum_value()\n",
    "\n",
    "# Create subplots for the two plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the progress of x\n",
    "axs[0].plot(x_progress, marker='o', linestyle='-', label='x Progress')\n",
    "axs[0].axhline(x_opt[0], color='red', linestyle='--', label='Optimal x')\n",
    "axs[0].set_title(\"Progress of x During Optimization\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"x Value\")\n",
    "axs[0].legend()\n",
    "axs[0].grid()\n",
    "\n",
    "# Plot the polynomial and overlay the x values over iterations\n",
    "for i, xi in enumerate(x_progress):\n",
    "    axs[1].text(xi, polynomial(np.array([xi])), f'{i}', fontsize=8, ha='right', va='bottom')\n",
    "axs[1].plot(x, polynomial(x), label='4th Order Polynomial')\n",
    "axs[1].scatter(x_progress, [polynomial(np.array([xi])) for xi in x_progress], color='red', label='x Progress', zorder=5)\n",
    "axs[1].axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "axs[1].axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "axs[1].axvline(x_opt[0], color='green', linestyle='--', label='Optimal x')\n",
    "axs[1].set_title(\"Polynomial with x Progress Over Iterations\")\n",
    "axs[1].set_xlabel(\"x\")\n",
    "axs[1].set_ylabel(\"f(x)\")\n",
    "axs[1].legend()\n",
    "axs[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal solution: x = {x_opt[0]}\")\n",
    "print(f\"Minimum value of the polynomial: f(x) = {min_f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2D function with two minima\n",
    "def two_minima_function(x, y):\n",
    "    return (x**2 + y**2) + np.sin(3 * x)\n",
    "\n",
    "# Generate the Z values for the function\n",
    "X, Y = np.meshgrid(np.linspace(-4, 4, 500), np.linspace(-4, 4, 500))\n",
    "Z = two_minima_function(X, Y)\n",
    "\n",
    "# Plot the 2D function\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "contour = ax.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "ax.set_title(\"2D Function with Two Minima\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "fig.colorbar(contour, ax=ax, label=\"f(x, y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2D objective function\n",
    "def objective_2d(x, grad):\n",
    "    if grad.size > 0:\n",
    "        grad[0] = 2 * x[0] + 3 * np.cos(3 * x[0])  # Partial derivative with respect to x\n",
    "        grad[1] = 2 * x[1]  # Partial derivative with respect to y\n",
    "    return (x[0]**2 + x[1]**2) + np.sin(3 * x[0])  # The 2D function\n",
    "\n",
    "# Create an optimizer object for 2D optimization\n",
    "opt_2d = nlopt.opt(nlopt.LD_MMA, 2)  # LD_MMA for Method of Moving Asymptotes\n",
    "\n",
    "# Set the objective function\n",
    "opt_2d.set_min_objective(objective_2d)\n",
    "\n",
    "# Set relative tolerance\n",
    "opt_2d.set_ftol_rel(1e-6)  # Set function tolerance\n",
    "opt_2d.set_xtol_rel(1e-6)\n",
    "\n",
    "# Set bounds for x and y\n",
    "x_min_2d = [-3.0, -3.0]\n",
    "x_max_2d = [3.0, 3.0]\n",
    "opt_2d.set_lower_bounds(x_min_2d)\n",
    "opt_2d.set_upper_bounds(x_max_2d)\n",
    "\n",
    "# Set initial guess\n",
    "x0_2d = [2.0, 2.0]\n",
    "\n",
    "# Optimize\n",
    "x_opt_2d = opt_2d.optimize(x0_2d)\n",
    "min_f_2d = opt_2d.last_optimum_value()\n",
    "\n",
    "print(f\"Optimal solution: x = {x_opt_2d[0]}, y = {x_opt_2d[1]}\")\n",
    "print(f\"Minimum value of the 2D function: f(x, y) = {min_f_2d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# List to store the progress of x and y during optimization\n",
    "xy_progress = []\n",
    "\n",
    "# Define a callback function to record x and y values\n",
    "def progress_callback_2d(x, grad):\n",
    "    xy_progress.append((x[0], x[1]))\n",
    "    return objective_2d(x, grad)\n",
    "\n",
    "# Set the callback function in the optimizer\n",
    "opt_2d.set_min_objective(progress_callback_2d)\n",
    "\n",
    "# Re-run the optimization\n",
    "xy_progress.clear()  # Clear previous progress\n",
    "x_opt_2d = opt_2d.optimize(x0_2d)\n",
    "min_f_2d = opt_2d.last_optimum_value()\n",
    "\n",
    "# Print the progress history\n",
    "print(\"Progress of solutions (x, y):\")\n",
    "for i, (x, y) in enumerate(xy_progress):\n",
    "    print(f\"Iteration {i}: x = {x}, y = {y}\")\n",
    "\n",
    "# Print the final optimal solution\n",
    "print(f\"Optimal solution: x = {x_opt_2d[0]}, y = {x_opt_2d[1]}\")\n",
    "print(f\"Minimum value of the 2D function: f(x, y) = {min_f_2d}\")\n",
    "\n",
    "\n",
    "# Plot the solutions history on the contour plot\n",
    "# Compute X, Y, Z for the 2D function\n",
    "X, Y = np.meshgrid(np.linspace(-4, 4, 500), np.linspace(-4, 4, 500))\n",
    "Z = (X**2 + Y**2) + np.sin(3 * X)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "ax.set_title(\"Optimization Progress on 2D Function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "fig.colorbar(contour, ax=ax, label=\"f(x, y)\")\n",
    "\n",
    "# Overlay the progress of solutions\n",
    "for i, (x, y) in enumerate(xy_progress):\n",
    "    ax.text(x, y + 0.1, str(i), fontsize=12, ha='center', va='bottom', color='white', zorder=3)\n",
    "    ax.scatter(x_opt_2d[0], x_opt_2d[1], color='lightgreen', marker='*', s=100, label='Optimal Point', zorder=7)\n",
    "ax.scatter(*zip(*xy_progress), color='red', label='Solution Progress', zorder=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap LD_MMA (Local Methods of Moving Asymptote) with Augmented Lagrangian (AUGLAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 2D objective function\n",
    "def objective_2d(x, grad):\n",
    "    if grad.size > 0:\n",
    "        grad[0] = 2 * x[0] + 3 * np.cos(3 * x[0])  # Partial derivative with respect to x\n",
    "        grad[1] = 2 * x[1]  # Partial derivative with respect to y\n",
    "    return (x[0]**2 + x[1]**2) + np.sin(3 * x[0])  # The 2D function\n",
    "\n",
    "# Create an optimizer object for 2D optimization\n",
    "base = nlopt.opt(nlopt.LD_MMA, 2)  # LD_MMA for Method of Moving Asymptotes\n",
    "base.set_ftol_rel(1e-6)  # Set function tolerance\n",
    "base.set_xtol_rel(1e-6)  # Set variable tolerance\n",
    "\n",
    "opt_2d = nlopt.opt(nlopt.AUGLAG, 2)\n",
    "opt_2d.set_min_objective(objective_2d)\n",
    "# Set bounds for x and y\n",
    "x_min_2d = [-3.0, -3.0]\n",
    "x_max_2d = [3.0, 3.0]\n",
    "opt_2d.set_lower_bounds(x_min_2d)\n",
    "opt_2d.set_upper_bounds(x_max_2d)\n",
    "opt_2d.set_local_optimizer(base)\n",
    "\n",
    "\n",
    "# Set bounds for x and y\n",
    "x_min_2d = [-3.0, -3.0]\n",
    "x_max_2d = [3.0, 3.0]\n",
    "opt_2d.set_lower_bounds(x_min_2d)\n",
    "opt_2d.set_upper_bounds(x_max_2d)\n",
    "\n",
    "# Set initial guess\n",
    "x0_2d = [2.0, 2.0]\n",
    "\n",
    "# Optimize\n",
    "x_opt_2d = opt_2d.optimize(x0_2d)\n",
    "min_f_2d = opt_2d.last_optimum_value()\n",
    "\n",
    "print(f\"Optimal solution: x = {x_opt_2d[0]}, y = {x_opt_2d[1]}\")\n",
    "print(f\"Minimum value of the 2D function: f(x, y) = {min_f_2d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# List to store the progress of x and y during optimization\n",
    "xy_progress = []\n",
    "\n",
    "# Define a callback function to record x and y values\n",
    "def progress_callback_2d(x, grad):\n",
    "    xy_progress.append((x[0], x[1]))\n",
    "    return objective_2d(x, grad)\n",
    "\n",
    "# Set the callback function in the optimizer\n",
    "opt_2d.set_min_objective(progress_callback_2d)\n",
    "\n",
    "# Re-run the optimization\n",
    "xy_progress.clear()  # Clear previous progress\n",
    "x_opt_2d = opt_2d.optimize(x0_2d)\n",
    "min_f_2d = opt_2d.last_optimum_value()\n",
    "\n",
    "# Print the progress history\n",
    "print(\"Progress of solutions (x, y):\")\n",
    "for i, (x, y) in enumerate(xy_progress):\n",
    "    print(f\"Iteration {i}: x = {x}, y = {y}\")\n",
    "\n",
    "# Print the final optimal solution\n",
    "print(f\"Optimal solution: x = {x_opt_2d[0]}, y = {x_opt_2d[1]}\")\n",
    "print(f\"Minimum value of the 2D function: f(x, y) = {min_f_2d}\")\n",
    "\n",
    "\n",
    "# Plot the solutions history on the contour plot\n",
    "# Compute X, Y, Z for the 2D function\n",
    "X, Y = np.meshgrid(np.linspace(-4, 4, 500), np.linspace(-4, 4, 500))\n",
    "Z = (X**2 + Y**2) + np.sin(3 * X)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "ax.set_title(\"Optimization Progress on 2D Function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "fig.colorbar(contour, ax=ax, label=\"f(x, y)\")\n",
    "\n",
    "# Overlay the progress of solutions\n",
    "for i, (x, y) in enumerate(xy_progress):\n",
    "    ax.text(x, y + 0.1, str(i), fontsize=12, ha='center', va='bottom', color='white', zorder=3)\n",
    "    ax.scatter(x_opt_2d[0], x_opt_2d[1], color='lightgreen', marker='*', s=100, label='Optimal Point', zorder=7)\n",
    "ax.scatter(*zip(*xy_progress), color='red', label='Solution Progress', zorder=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective: f(x,y) = (1 - x)^2 + 100*(y - x^2)^2\n",
    "def f_obj(x, grad):\n",
    "    x1, x2 = x\n",
    "    if grad.size > 0:\n",
    "        # df/dx1 = 2(x-1) - 400 x (y - x^2)\n",
    "        grad[0] = 2.0*(x1 - 1.0) - 400.0*x1*(x2 - x1**2)\n",
    "        # df/dx2 = 200 (y - x^2)\n",
    "        grad[1] = 200.0*(x2 - x1**2)\n",
    "    return (1.0 - x1)**2 + 100.0*(x2 - x1**2)**2\n",
    "\n",
    "# Define equality constraints x1 + x2 - 1 = 0\n",
    "def equality_constraint(x, grad):\n",
    "    # c1(x) = x1 + x2 - 1 = 0\n",
    "    if grad.size > 0:\n",
    "        # grad is a flattened array, so we set gradients for both constraints\n",
    "        # grad[0] and grad[1] for c1, grad[2] and grad[3] for c2\n",
    "        grad[0] = 1.0  # dc1/dx1\n",
    "        grad[1] = 1.0  # dc1/dx2\n",
    "    return x[0] + x[1] - 1.0\n",
    "\n",
    "# Define equality constraints x1**2 + x2**2 - 4.0*x1 <= 0\n",
    "def inequality_constraint(x, grad):\n",
    "    # c1(x) = x1**2 + x2**2 - 4*x1 <= 0\n",
    "    if grad.size > 0:\n",
    "        grad[0] = 2.0*x[0] - 4.0\n",
    "        grad[1] = 2.0*x[1]\n",
    "    return x[0]**2 + x[1]**2 - 4.0*x[0]\n",
    "    \n",
    "\n",
    "# Set up the optimizer\n",
    "base = nlopt.opt(nlopt.LD_MMA, 2)\n",
    "base.set_ftol_rel(1e-4)\n",
    "base.set_xtol_rel(1e-4)\n",
    "\n",
    "opt_aug = nlopt.opt(nlopt.AUGLAG, 2)\n",
    "opt_aug.set_min_objective(f_obj)\n",
    "opt_aug.set_local_optimizer(base)\n",
    "opt_aug.set_lower_bounds([-5.0, -5.0])\n",
    "opt_aug.set_upper_bounds([5.0, 5.0])\n",
    "\n",
    "opt_aug.add_equality_constraint(equality_constraint_1, 1e-4)\n",
    "opt_aug.add_equality_constraint(equality_constraint_2, 1e-4)\n",
    "opt_aug.add_inequality_constraint(inequality_constraint, 1e-4)\n",
    "\n",
    "x0_aug = [4.0, 4.0]\n",
    "x_opt_aug = opt_aug.optimize(x0_aug)\n",
    "min_f_aug = opt_aug.last_optimum_value()\n",
    "\n",
    "print(f\"Optimal solution with equality constraint: x = {x_opt_aug[0]}, y = {x_opt_aug[1]}\")\n",
    "print(f\"Minimum value of f_obj: f(x, y) = {min_f_aug}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# List to store the progress of x and y during optimization\n",
    "xy_progress = []\n",
    "\n",
    "# Define a callback function to record x and y values\n",
    "def progress_callback_2d(x, grad):\n",
    "    xy_progress.append((x[0], x[1]))\n",
    "    return f_obj(x, grad)\n",
    "\n",
    "# Set the callback function in the optimizer\n",
    "opt_aug.set_min_objective(progress_callback_2d)\n",
    "\n",
    "# Re-run the optimization\n",
    "xy_progress.clear()  # Clear previous progress\n",
    "x_opt_2d = opt_aug.optimize(x0_2d)\n",
    "min_f_2d = opt_aug.last_optimum_value()\n",
    "\n",
    "# Print the progress history\n",
    "print(\"Progress of solutions (x, y):\")\n",
    "for i, (x, y) in enumerate(xy_progress):\n",
    "    if i%100==0:\n",
    "        print(f\"Iteration {i}: x = {x}, y = {y}\")\n",
    "\n",
    "# Print the final optimal solution\n",
    "print(f\"Optimal solution: x = {x_opt_2d[0]}, y = {x_opt_2d[1]}\")\n",
    "print(f\"Minimum value of the 2D function: f(x, y) = {min_f_2d}\")\n",
    "\n",
    "\n",
    "# Plot the solutions history on the contour plot\n",
    "# Compute X, Y, Z for the 2D function\n",
    "X, Y = np.meshgrid(np.linspace(-10, 10, 500), np.linspace(-10, 10, 500))\n",
    "Z = (1 - X)**2 + 100*(Y - X**2)**2\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
    "ax.set_title(\"Optimization Progress on 2D Function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "fig.colorbar(contour, ax=ax, label=\"f(x, y)\")\n",
    "\n",
    "# Overlay the progress of solutions\n",
    "for i, (x, y) in enumerate(xy_progress):\n",
    "    if (i % 10 == 0)|(i<20):\n",
    "        ax.text(x, y + 0.1, str(i), fontsize=10, ha='center', va='bottom', color='white', zorder=3)\n",
    "ax.scatter(x_opt_2d[0], x_opt_2d[1], color='lightgreen', marker='*', s=100, label='Optimal Point', zorder=7)\n",
    "ax.scatter(*zip(*[(x, y) for i, (x, y) in enumerate(xy_progress) if ((i % 10 == 0)|(i<20))]), color='red', label='Solution Progress', zorder=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_opt, y_opt = x_opt_2d\n",
    "print('check constraints at optimal solution:')\n",
    "print(\"Constraint 1 satisfied:\", \"YES\" if np.isclose(equality_constraint_1(x_opt_2d, np.zeros(2)), 0.0) else \"NO\")\n",
    "print(\"Constraint 2 satisfied:\", \"YES\" if np.isclose(equality_constraint_2(x_opt_2d, np.zeros(2)), 0.0) else \"NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of combining Augmented Lagrangian (AUGLAG) & Methods of Moving Asymptote (LD_MMA)\n",
    "- Ability to implement equalities constraints directly (via AUGLAG). Recall: if using MMA alone, we need to break equality constraint into 2 inequalities constraints.\n",
    "- Robustness to scaling/conflicts: If constraints fight each other or are poorly scaled, AUGLAGâ€™s multiplier/penalty updates adapt better than a pure penalty or ad-hoc MMA setup.\n",
    "- Ability to tune tolerance separately:\n",
    "    - Tune outer-tolerance for constraints (via AUGLAG optimization)\n",
    "    - Tune inner-tolerance for sub-problem solver (via LD_MMA optimization)\n",
    "- Most importantly, AUGLAG+LD_MMA will make sure that the 'optimal' solutions will satisfy constraints with high accuracy! In other words, optimal solutions found by AUGLAG+LD_MMA will definitely be feasible w.r.t constraints.\n",
    "    - MMA alone: try to find optimal solution with O.K. mindset of missing constraints by a bit\n",
    "    - AUGLAG+MMA: find optimal solution that will align very well with constraints!\n",
    "- Mathematically, AUGLAG+MMA:\n",
    "    - calls inner optimizer (MMA) to minimize augmented Lagrangian\n",
    "    - outer-update: after finding the next optimal solution, it looks for constraints violations. If a constraint is violated, then increase $\\lambda_i$ so that it makes next inner optimizing task works harder. On the other hand, if constraint is satisfied, it relaxes things by a bit and focus on optimizing objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
